{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install faiss-gpu\n",
    "# !pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import embed\n",
    "\n",
    "from memorizing_transformers import MemorizingModel, MemorizingLMHeadModel\n",
    "import random\n",
    "import tqdm\n",
    "import gzip\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_BATCHES = int(1e5)\n",
    "BATCH_SIZE = 8\n",
    "SEQ_LEN = 512\n",
    "SEGMENTS = 5\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_GRAD_CLIP_NORM = 0.5\n",
    "\n",
    "VALIDATE_EVERY  = 100\n",
    "GENERATE_EVERY  = 500\n",
    "GENERATE_LENGTH = 512\n",
    "\n",
    "# helpers\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def decode_token(token):\n",
    "    return str(chr(max(32, token)))\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    return ''.join(list(map(decode_token, tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# with gzip.open('./data/enwik8.gz',\"rt\",encoding=\"utf-8\") as file:\n",
    "#     text = file.read()\n",
    "#     X = tokenizer(text).input_ids\n",
    "# with open('./data/enwik8_token.pickle', 'wb') as file:\n",
    "#     pickle.dump(X, file)\n",
    "\n",
    "with open('./data/enwik8_token.pickle', 'rb') as file:\n",
    "    X = pickle.load(file)\n",
    "\n",
    "tr_num = len(X) // 2\n",
    "trX, vaX = np.split(X, [tr_num])\n",
    "data_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate GPT-like decoder model\n",
    "# model = MemorizingTransformer(\n",
    "#     num_tokens = 256, # int8 tokens, 32k (x128 = 2^7d) in the paper\n",
    "#     dim = 512, # dim of token in embedding space, 1024 (x2) in the paper\n",
    "#     depth = 8, # 12 \n",
    "#     memorizing_layers = 4, # 9 in the paper\n",
    "#     max_knn_memories = 512 * 15, \n",
    "#     num_retrieved_memories = 32, \n",
    "#     xl_memory_layers = (7, 8),\n",
    "#     xl_max_memories = 512,\n",
    "# )\n",
    "\n",
    "model = MemorizingLMHeadModel.from_pretrained('gpt2').to(DEVICE)\n",
    "\n",
    "# prepare enwik8 data\n",
    "\n",
    "class TextSamplerDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rand_start = torch.randint(0, self.data.size(0) - self.seq_len, (1,))\n",
    "        full_seq = self.data[rand_start: rand_start + self.seq_len].long()\n",
    "        return full_seq.to(DEVICE)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0) // self.seq_len\n",
    "\n",
    "# dataset and dataloader\n",
    "\n",
    "train_dataset = TextSamplerDataset(data_train, SEQ_LEN * SEGMENTS)\n",
    "train_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE, drop_last = True))\n",
    "valid_dataset = TextSamplerDataset(data_val, SEQ_LEN * SEGMENTS)\n",
    "valid_loader = cycle(DataLoader(valid_dataset, batch_size = BATCH_SIZE, drop_last = True))\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(train_loader)\n",
    "next(valid_loader)\n",
    "print(f\"{model.num_parameters()//1e6}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_prefix = \"History of Rome dates back to\"\n",
    " \n",
    "# input_ids = tokenizer.encode(\n",
    "#     sentence_prefix,\n",
    "#     add_special_tokens=False,\n",
    "#     return_tensors=\"pt\",\n",
    "# ).to(DEVICE)\n",
    " \n",
    "# output_ids = model.generate(\n",
    "#     input_ids=input_ids,\n",
    "#     do_sample=True,\n",
    "#     max_length=50,  # desired output sentence length\n",
    "#     pad_token_id=model.config.eos_token_id,\n",
    "# )[0].tolist()\n",
    " \n",
    "# generated_text = tokenizer.decode(\n",
    "#     output_ids,\n",
    "#     clean_up_tokenization_spaces=True)\n",
    " \n",
    "# print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "for i in tqdm.tqdm(range(NUM_BATCHES), mininterval = 10., desc = 'training'):\n",
    "    model.train()\n",
    "\n",
    "    data = next(train_loader)\n",
    "\n",
    "    train_loss = 0.\n",
    "    seq = data\n",
    "\n",
    "    with model.knn_memories_context(batch_size = BATCH_SIZE, num_heads=12) as knn_memories:\n",
    "\n",
    "        for seq_segment in seq.chunk(SEGMENTS, dim = -1):\n",
    "            result = model(\n",
    "                input_ids = seq_segment,\n",
    "                labels = seq_segment,\n",
    "                knn_memories = knn_memories\n",
    "            )\n",
    "\n",
    "            loss = result.loss\n",
    "\n",
    "            train_loss += loss.item() / SEGMENTS\n",
    "            (loss / SEGMENTS).backward()\n",
    "\n",
    "    #         output_ids = model.generate(\n",
    "    #             input_ids=seq_segment,\n",
    "    #             do_sample=True,\n",
    "    #             max_length=20,  # desired output sentence length\n",
    "    #             pad_token_id=model.config.eos_token_id,\n",
    "    #         )[0].tolist()\n",
    "\n",
    "    #         generated_text = tokenizer.decode(\n",
    "    #             output_ids,\n",
    "    #             clean_up_tokenization_spaces=True)\n",
    "\n",
    "    #         print(generated_text)\n",
    "\n",
    "\n",
    "        print(f'training loss: {train_loss}')\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_CLIP_NORM)\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "    if not (i % VALIDATE_EVERY):\n",
    "        model.eval()\n",
    "\n",
    "        valid_data = next(valid_loader)\n",
    "        valid_loss = 0.\n",
    "\n",
    "        with torch.no_grad():\n",
    "            seq = data\n",
    "            \n",
    "            for seq_segment in seq.chunk(SEGMENTS, dim = -1):\n",
    "                \n",
    "                result = model(\n",
    "                    input_ids = seq_segment,\n",
    "                    labels = seq_segment,\n",
    "                )\n",
    "\n",
    "                loss = result.loss\n",
    "\n",
    "                valid_loss += loss.item() / SEGMENTS\n",
    "\n",
    "        print(f'valid loss: {valid_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
