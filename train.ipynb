{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install faiss-gpu\n",
    "# !pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import embed\n",
    "\n",
    "from memorizing_transformers import MemorizingModel, MemorizingLMHeadModel\n",
    "import random\n",
    "import tqdm\n",
    "import gzip\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_BATCHES = int(1e5)\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LEN = 512\n",
    "SEGMENTS = 5\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_GRAD_CLIP_NORM = 0.5\n",
    "\n",
    "VALIDATE_EVERY  = 10\n",
    "GENERATE_EVERY  = 500\n",
    "GENERATE_LENGTH = 512\n",
    "\n",
    "# helpers\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def decode_token(token):\n",
    "    return str(chr(max(32, token)))\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    return ''.join(list(map(decode_token, tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# with gzip.open('./data/enwik8.gz',\"rt\",encoding=\"utf-8\") as file:\n",
    "#     text = file.read()\n",
    "#     X = tokenizer(text).input_ids\n",
    "# with open('./data/enwik8_token.pickle', 'wb') as file:\n",
    "#     pickle.dump(X, file)\n",
    "\n",
    "with open('./data/enwik8_token.pickle', 'rb') as file:\n",
    "    X = pickle.load(file)\n",
    "\n",
    "tr_num = len(X) // 2\n",
    "trX, vaX = np.split(X, [tr_num])\n",
    "data_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(X)/10**6} million tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = MemorizingLMHeadModel.from_pretrained('checkpoint/checkpoint_499').to(DEVICE)\n",
    "model = MemorizingLMHeadModel.from_pretrained('checkpoint/memorizing_checkpoint_499').to(DEVICE)\n",
    "#model = MemorizingLMHeadModel.from_pretrained('gpt2').to(DEVICE)\n",
    "\n",
    "# prepare enwik8 data\n",
    "\n",
    "class TextSamplerDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rand_start = torch.randint(0, self.data.size(0) - self.seq_len, (1,))\n",
    "        full_seq = self.data[rand_start: rand_start + self.seq_len].long()\n",
    "        \n",
    "        return full_seq.to(DEVICE)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0) // self.seq_len\n",
    "\n",
    "class RepeatedTextSamplerDataset(Dataset):\n",
    "    def __init__(self,data,single_seq_len,num_segments):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.single_seq_len = single_seq_len\n",
    "        self.num_segments = num_segments\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        rand_start = torch.randint(0,self.data.size(0) - self.single_seq_len, (1,))\n",
    "        full_seq = self.data[rand_start: rand_start + self.single_seq_len].long()\n",
    "        full_seq = full_seq.repeat(self.num_segments)\n",
    "        return full_seq.to(DEVICE)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "    \n",
    "# dataset and dataloader\n",
    "\n",
    "#train_dataset = TextSamplerDataset(data_train, SEQ_LEN * SEGMENTS)\n",
    "train_dataset = RepeatedTextSamplerDataset(data_train, SEQ_LEN, SEGMENTS)\n",
    "train_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE, drop_last = True))\n",
    "\n",
    "#valid_dataset = TextSamplerDataset(data_val, SEQ_LEN * SEGMENTS)\n",
    "valid_dataset = RepeatedTextSamplerDataset(data_val, SEQ_LEN, SEGMENTS)\n",
    "valid_loader = cycle(DataLoader(valid_dataset, batch_size = BATCH_SIZE, drop_last = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(train_loader)\n",
    "next(valid_loader)\n",
    "print(f\"{model.num_parameters()//1e6}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam([param for (name,param) in model.named_parameters() if name == \"transformer.h.5.attn.knn_attention_ratio\"], lr = LEARNING_RATE)\n",
    "#optim = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_prefix = \"\"\"\n",
    "# History of Rome dates back to\n",
    "# \"\"\"\n",
    "# input_ids = tokenizer.encode(\n",
    "#     sentence_prefix,\n",
    "#     add_special_tokens=False,\n",
    "#     return_tensors=\"pt\",\n",
    "# ).to(DEVICE)\n",
    "\n",
    "# with open('./data/history_rome.txt',\"rt\",encoding=\"utf-8\") as file:\n",
    "#     sample_text = file.read()\n",
    "#     sample_X = tokenizer(sample_text).input_ids\n",
    "#     sample_X = torch.from_numpy(np.array([sample_X]))\n",
    "    \n",
    "# seq=sample_X[:,0:SEGMENTS*SEQ_LEN].long().to(DEVICE)\n",
    "    \n",
    "# with model.knn_memories_context(batch_size = 1, num_heads=12) as knn_memories:\n",
    "    \n",
    "#     for index_seg, seq_segment in enumerate(seq.chunk(SEGMENTS, dim = -1)):     \n",
    "#         with torch.no_grad():\n",
    "#             result = model(\n",
    "#                 input_ids = seq_segment,\n",
    "#                 labels = seq_segment.clone(),\n",
    "#                 knn_memories = knn_memories,\n",
    "#                 knn_memory_layer= 5\n",
    "#             )\n",
    "    \n",
    "#     output_ids = model.generate(\n",
    "#         input_ids=input_ids,\n",
    "#         do_sample=True,\n",
    "#         max_length=300,  # desired output sentence length\n",
    "#         pad_token_id=model.config.eos_token_id,\n",
    "#         knn_memories = knn_memories,\n",
    "#         knn_memory_layer= 5,\n",
    "        \n",
    "#     )[0].tolist()\n",
    "\n",
    "#     generated_text = tokenizer.decode(\n",
    "#         output_ids,\n",
    "#         clean_up_tokenization_spaces=True)\n",
    "    \n",
    "#     print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "knn_memory_layer = 5\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "tb = SummaryWriter()\n",
    "\n",
    "NUM_BATCHES=500\n",
    "\n",
    "for i in tqdm.tqdm(range(NUM_BATCHES), mininterval = 10., desc = 'training'):\n",
    "    model.train()\n",
    "\n",
    "    data = next(train_loader)\n",
    "\n",
    "    train_loss = 0.\n",
    "    seq = data\n",
    "    \n",
    "    with model.knn_memories_context(batch_size = BATCH_SIZE, num_heads=12) as knn_memories:\n",
    "        \n",
    "        for index_seg, seq_segment in enumerate(seq.chunk(SEGMENTS, dim = -1)):\n",
    "            result = model(\n",
    "                input_ids = seq_segment,\n",
    "                labels = seq_segment,\n",
    "                knn_memories = knn_memories,\n",
    "                knn_memory_layer=knn_memory_layer\n",
    "            )\n",
    "\n",
    "            loss = result.loss\n",
    "            print(f'training loss for {index_seg}th segment: {loss}')\n",
    "\n",
    "            train_loss += loss.item() / SEGMENTS\n",
    "            (loss / SEGMENTS).backward()\n",
    "\n",
    "\n",
    "        print(f'training loss: {train_loss}')\n",
    "        tb.add_scalars(\"Loss\", {\"training_loss\": train_loss}, i*SEGMENTS*BATCH_SIZE)\n",
    "\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_CLIP_NORM)\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "    # if not (i % VALIDATE_EVERY):\n",
    "    if True:\n",
    "        model.eval()\n",
    "\n",
    "        valid_data = next(valid_loader)\n",
    "        valid_loss = 0.\n",
    "\n",
    "        with torch.no_grad(), model.knn_memories_context(batch_size = BATCH_SIZE, num_heads=12) as knn_memories:\n",
    "            seq = valid_data\n",
    "            \n",
    "            for index_seg, seq_segment in enumerate(seq.chunk(SEGMENTS, dim = -1)):\n",
    "                \n",
    "                result = model(\n",
    "                    input_ids = seq_segment,\n",
    "                    labels = seq_segment,\n",
    "                    knn_memories = knn_memories,\n",
    "                    knn_memory_layer=knn_memory_layer\n",
    "                )\n",
    "\n",
    "                loss = result.loss\n",
    "                tb.add_scalars(\"Segment Loss\", {f\"segment_loss_{index_seg}\": loss}, i*SEGMENTS*BATCH_SIZE)\n",
    "                tb.add_scalars(\"Segment Loss Perplexity\", {f\"segment_loss_perplexity_{index_seg}\": torch.exp(loss)}, i*SEGMENTS*BATCH_SIZE)\n",
    "\n",
    "                valid_loss += loss.item() / SEGMENTS\n",
    "\n",
    "        print(f'valid loss: {valid_loss}')\n",
    "        tb.add_scalars(\"Loss\", {\"validation_loss\": valid_loss}, i*SEGMENTS*BATCH_SIZE)\n",
    "        tb.add_histogram(\"memorizing_attention_ratio\", torch.sigmoid(model.transformer.h[5].attn.knn_attention_ratio*100), i*SEGMENTS*BATCH_SIZE)\n",
    "\n",
    "    if not ((i+1) % 100):\n",
    "        model.save_pretrained(f\"./checkpoint/memorizing_checkpoint_{i}\")\n",
    "\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to python train.ipynb\n",
    "# !nohup python train.py > output.txt ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
